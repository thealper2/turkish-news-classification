{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13427329,"sourceType":"datasetVersion","datasetId":7486801}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:58:37.385250Z","iopub.execute_input":"2025-11-20T08:58:37.385483Z","iopub.status.idle":"2025-11-20T08:58:39.019726Z","shell.execute_reply.started":"2025-11-20T08:58:37.385459Z","shell.execute_reply":"2025-11-20T08:58:39.018953Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/turkish-news-dataset/new_data.csv\n/kaggle/input/turkish-news-dataset/cleaned.csv\n/kaggle/input/turkish-news-dataset/glove.twitter.27B.100d.txt\n/kaggle/input/turkish-news-dataset/stop-words.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -Uqq transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T18:24:14.902626Z","iopub.execute_input":"2025-11-19T18:24:14.903372Z","iopub.status.idle":"2025-11-19T18:24:28.278619Z","shell.execute_reply.started":"2025-11-19T18:24:14.903342Z","shell.execute_reply":"2025-11-19T18:24:28.277580Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport emoji\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall\nfrom collections import Counter\nfrom datetime import datetime\nimport csv\nimport gc\n\nfrom transformers import AutoTokenizer, TFAutoModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# BELLEK OPTİMİZASYONU: Mixed Precision kullan\nfrom tensorflow.keras import mixed_precision\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\nprint(f'Compute dtype: {policy.compute_dtype}')\nprint(f'Variable dtype: {policy.variable_dtype}')\n\n# GPU kontrol ve bellek ayarları\ndef check_gpu():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        print(f\"{len(gpus)} GPU bulundu:\")\n        for gpu in gpus:\n            print(f\"  {gpu}\")\n        for gpu in gpus:\n            try:\n                # BELLEK OPTİMİZASYONU: Memory growth + limit\n                tf.config.experimental.set_memory_growth(gpu, True)\n                # İsteğe bağlı: GPU belleğini sınırla (örn. 8GB)\n                # tf.config.set_logical_device_configuration(\n                #     gpu,\n                #     [tf.config.LogicalDeviceConfiguration(memory_limit=8192)]\n                # )\n                print(f\"GPU bellek büyümesi {gpu} için etkinleştirildi\")\n            except RuntimeError as e:\n                print(f\"GPU bellek büyümesi {gpu} için ayarlanamadı: {e}\")\n        return True\n    else:\n        print(\"GPU bulunamadı, işlemler CPU üzerinde gerçekleştirilecek\")\n        return False\n\n# F1 metriği\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(name=name, **kwargs)\n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.argmax(y_pred, axis=1)\n        y_true = tf.argmax(y_true, axis=1)\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n\n    def reset_state(self):\n        self.precision.reset_state()\n        self.recall.reset_state()\n\n    def result(self):\n        p = self.precision.result()\n        r = self.recall.result()\n        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n\n# Türkçe stopword temizliği\ndef load_turkish_stopwords(stopwords_path):\n    stoplist = []\n    try:\n        with open(stopwords_path, \"r\", encoding='utf-8') as f:\n            for line in f:\n                if line.strip() and line.strip()[0:1] != \"#\":\n                    for word in line.split():\n                        stoplist.append(word)\n    except Exception as e:\n        print(f\"Stopword dosyası yüklenirken hata: {e}\")\n    return stoplist\n\n# Metin temizleme fonksiyonu\ndef clean_turkish_text(text, stoplist=None):\n    text = text.lower()\n    text = re.sub(\"\\n\", \" \", text)\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'www\\S+', '', text)\n    text = re.sub(r'\\S+@\\S+', '', text)\n    text = emoji.replace_emoji(text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'[0-9]+', '', text)\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    \n    if stoplist:\n        words = text.split()\n        text = ' '.join([word for word in words if word not in stoplist])\n    \n    text = ' '.join(text.split())\n    return text\n\ndef create_directories():\n    dirs = [\n        \"outputs\",\n        \"outputs/models\",\n        \"outputs/history\",\n        \"outputs/images\",\n        \"outputs/confusion_matrices\"\n    ]\n    for dir_path in dirs:\n        os.makedirs(dir_path, exist_ok=True)\n    print(\"Dizin yapısı hazırlandı\")\n\n# BELLEK OPTİMİZASYONU: Veri setini küçük parçalarda yükle\ndef load_and_preprocess_data(data_path, text_column=\"Haber Gövdesi Cleaned\", \n                             label_column=\"Sınıf\", stopwords_path=None, \n                             max_length=128, sample_size=None):  # max_length 256->128\n    \"\"\"Veri setini yükler ve ön işler\"\"\"\n    print(f\"Veri seti yükleniyor: {data_path}\")\n    \n    try:\n        # BELLEK OPTİMİZASYONU: Sadece gerekli sütunları yükle\n        df = pd.read_csv(data_path, usecols=[text_column, label_column], encoding='utf-8')\n        \n        # BELLEK OPTİMİZASYONU: Test için küçük sample\n        if sample_size:\n            print(f\"Veri seti {sample_size} örneğe düşürülüyor (test için)\")\n            df = df.sample(n=min(sample_size, len(df)), random_state=42)\n        \n        df = df[[text_column, label_column]].copy()\n        df.columns = ['text', 'label']\n    except Exception as e:\n        raise ValueError(f\"Veri yüklenirken hata oluştu: {str(e)}\")\n    \n    label_encoder = LabelEncoder()\n    y_encoded = label_encoder.fit_transform(df['label'])\n    num_classes = len(label_encoder.classes_)\n    \n    print(f\"Sınıf sayısı: {num_classes}\")\n    print(f\"Sınıflar: {label_encoder.classes_}\")\n    \n    X = df['text']\n    print(f'NaN sayısı: {X.isna().sum()}')\n    X = X.dropna()\n    y_encoded = y_encoded[X.index]\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded\n    )\n\n    X_train = X_train.reset_index(drop=True)\n    X_test = X_test.reset_index(drop=True)\n    \n    # One-hot encoding\n    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)\n    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes)\n    \n    # Sınıf ağırlıklarını hesapla\n    class_counts = np.bincount(y_train)\n    total = len(y_train)\n    class_weights = {i: np.sqrt(total / (num_classes * count)) for i, count in enumerate(class_counts)}\n    \n    print(f\"Sınıf ağırlıkları: {class_weights}\")\n    print(f\"Eğitim seti: {len(X_train)}, Test seti: {len(X_test)}\")\n    \n    # BELLEK TEMİZLİĞİ\n    del df\n    gc.collect()\n    \n    return X_train, X_test, y_train_onehot, y_test_onehot, class_weights, label_encoder, num_classes, y_test\n\n# BELLEK OPTİMİZASYONU: Generator kullan\ndef data_generator(texts, labels, tokenizer, batch_size, max_length):\n    \"\"\"Batch batch veri üretir - bellek dostu\"\"\"\n    num_samples = len(texts)\n    indices = np.arange(num_samples)\n    \n    while True:\n        np.random.shuffle(indices)\n        \n        for start_idx in range(0, num_samples, batch_size):\n            end_idx = min(start_idx + batch_size, num_samples)\n            batch_indices = indices[start_idx:end_idx]\n            \n            batch_texts = [texts.iloc[i] for i in batch_indices]\n            batch_labels = labels[batch_indices]\n            \n            # Tokenize\n            encodings = tokenizer(\n                batch_texts,\n                max_length=max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='tf'\n            )\n            \n            yield (encodings['input_ids'], encodings['attention_mask']), batch_labels\n\n# Custom BERT Layer\nclass TFBertLayer(tf.keras.layers.Layer):\n    def __init__(self, bert_model_name, **kwargs):\n        super().__init__(**kwargs)\n        self.bert_model_name = bert_model_name\n        self.bert = TFAutoModel.from_pretrained(bert_model_name)\n        self.bert.trainable = False  # BELLEK OPTİMİZASYONU: BERT'i dondur\n        self.hidden_size = self.bert.config.hidden_size\n    \n    def call(self, inputs):\n        input_ids, attention_mask = inputs\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.pooler_output\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.hidden_size)\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\"bert_model_name\": self.bert_model_name})\n        return config\n\n# BELLEK OPTİMİZASYONU: Daha küçük model\ndef create_bert_functional_model(num_classes, bert_model_name=\"dbmdz/bert-base-turkish-uncased\", \n                                max_length=128):\n    \"\"\"Functional API ile BERT modeli\"\"\"\n    print(f\"Functional BERT modeli oluşturuluyor: {bert_model_name}\")\n    \n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n    \n    # Input layers\n    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n    \n    # Custom BERT layer\n    bert_output = TFBertLayer(bert_model_name, name='bert_layer')([input_ids, attention_mask])\n    \n    # BELLEK OPTİMİZASYONU: Daha küçük katmanlar\n    x = tf.keras.layers.Dropout(0.3)(bert_output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)  # 256->128\n    x = tf.keras.layers.Dropout(0.2)(x)\n    # İkinci dense layer'ı kaldır\n    outputs = tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n    \n    return model, tokenizer\n\ndef create_tf_dataset_simple(texts, labels, tokenizer, batch_size, max_length, shuffle=True, num_classes=9):\n    \"\"\"Generator kullanarak bellek dostu dataset - FIXED\"\"\"\n    \n    class TextGenerator:\n        def __init__(self, texts, labels, tokenizer, max_length, num_classes):\n            self.texts = texts\n            self.labels = labels  # Bu zaten one-hot encoded\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n            self.num_classes = num_classes\n        \n        def __len__(self):\n            return len(self.texts)\n        \n        def __call__(self):\n            for i in range(len(self.texts)):\n                text = str(self.texts.iloc[i])\n                label = self.labels[i]  # Zaten one-hot encoded (shape: (9,))\n                \n                # Sadece bir örnek tokenize et\n                encoding = self.tokenizer(\n                    text,\n                    max_length=self.max_length,\n                    padding='max_length',\n                    truncation=True,\n                    return_tensors='tf'\n                )\n                \n                # One-hot encoding YAPMA - zaten one-hot!\n                # label artık direkt kullanılabilir\n                \n                yield {\n                    'input_ids': encoding['input_ids'][0],\n                    'attention_mask': encoding['attention_mask'][0]\n                }, label  # Direkt label kullan (zaten one-hot)\n    \n    print(f\"Generator dataset oluşturuluyor... ({len(texts)} örnek)\")\n    \n    generator = TextGenerator(texts, labels, tokenizer, max_length, num_classes)\n    \n    # Output signature tanımla - ONE-HOT LABELS\n    output_signature = (\n        {\n            'input_ids': tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n        },\n        tf.TensorSpec(shape=(num_classes,), dtype=tf.float32)  # One-hot labels\n    )\n    \n    dataset = tf.data.Dataset.from_generator(\n        generator,\n        output_signature=output_signature\n    )\n    \n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=min(1000, len(texts)))\n    \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    # Output formatını düzenle\n    def reformat(features, labels):\n        return (features['input_ids'], features['attention_mask']), labels\n    \n    dataset = dataset.map(reformat)\n    \n    print(f\"Generator dataset hazır: {len(texts)} örnek, batch_size={batch_size}\")\n    return dataset\n\ndef plot_confusion_matrix(y_true, y_pred, classes, model_name, normalize=False, cmap=plt.cm.Blues):\n    \"\"\"\n    Confusion matrix çizer ve kaydeder\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        title = 'Normalized Confusion Matrix'\n    else:\n        title = 'Confusion Matrix'\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', cmap=cmap,\n                xticklabels=classes, yticklabels=classes, cbar_kws={'shrink': 0.8})\n    \n    plt.title(f'{title} - {model_name}', fontsize=16, fontweight='bold')\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    \n    # Kaydet\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"outputs/confusion_matrices/{model_name}_confusion_matrix_{timestamp}.png\"\n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"Confusion matrix kaydedildi: {filename}\")\n    return cm\n\ndef calculate_macro_metrics(y_true, y_pred, label_encoder, model_name):\n    \"\"\"\n    Macro precision, recall, f1-score ve accuracy hesaplar\n    \"\"\"\n    # Metrikleri hesapla\n    accuracy = accuracy_score(y_true, y_pred)\n    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n    \n    # Detaylı classification report\n    class_report = classification_report(y_true, y_pred, \n                                        target_names=label_encoder.classes_,\n                                        zero_division=0)\n    \n    # Sonuçları yazdır\n    print(\"\\n\" + \"=\"*60)\n    print(\"MACRO METRİKLER - DETAYLI SONUÇLAR\")\n    print(\"=\"*60)\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision (Macro): {precision_macro:.4f}\")\n    print(f\"Recall (Macro): {recall_macro:.4f}\")\n    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(class_report)\n    \n    # CSV dosyasına kaydet\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    metrics_filename = f\"outputs/{model_name}_macro_metrics_{timestamp}.csv\"\n    \n    metrics_data = {\n        'Model': [model_name],\n        'Timestamp': [timestamp],\n        'Accuracy': [accuracy],\n        'Precision_Macro': [precision_macro],\n        'Recall_Macro': [recall_macro],\n        'F1_Score_Macro': [f1_macro]\n    }\n    \n    metrics_df = pd.DataFrame(metrics_data)\n    metrics_df.to_csv(metrics_filename, index=False, encoding='utf-8')\n    print(f\"\\nMacro metrikler kaydedildi: {metrics_filename}\")\n    \n    # Classification report'u txt dosyasına kaydet\n    report_filename = f\"outputs/{model_name}_classification_report_{timestamp}.txt\"\n    with open(report_filename, 'w', encoding='utf-8') as f:\n        f.write(f\"Model: {model_name}\\n\")\n        f.write(f\"Timestamp: {timestamp}\\n\")\n        f.write(\"=\"*50 + \"\\n\")\n        f.write(\"MACRO METRİKLER\\n\")\n        f.write(\"=\"*50 + \"\\n\")\n        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n        f.write(f\"Precision (Macro): {precision_macro:.4f}\\n\")\n        f.write(f\"Recall (Macro): {recall_macro:.4f}\\n\")\n        f.write(f\"F1-Score (Macro): {f1_macro:.4f}\\n\\n\")\n        f.write(\"DETAYLI CLASSIFICATION REPORT:\\n\")\n        f.write(\"=\"*50 + \"\\n\")\n        f.write(class_report)\n    \n    print(f\"Classification report kaydedildi: {report_filename}\")\n    \n    return {\n        'accuracy': accuracy,\n        'precision_macro': precision_macro,\n        'recall_macro': recall_macro,\n        'f1_macro': f1_macro\n    }\n\ndef evaluate_model_with_metrics(model, X_test, y_test, tokenizer, label_encoder, model_name, batch_size=8, max_length=128):\n    \"\"\"\n    Modeli değerlendirir ve confusion matrix + macro metrikleri hesaplar\n    \"\"\"\n    print(\"Model değerlendirmesi başlıyor...\")\n    \n    # Test dataset oluştur\n    test_dataset = create_tf_dataset_simple(X_test, y_test, tokenizer, batch_size, max_length, shuffle=False)\n    \n    # Tahminleri yap\n    print(\"Tahminler yapılıyor...\")\n    y_pred_proba = model.predict(test_dataset, verbose=1)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    \n    # Gerçek etiketleri al (one-hot'den normal forma çevir)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Confusion matrix çiz\n    print(\"Confusion matrix oluşturuluyor...\")\n    cm = plot_confusion_matrix(y_true, y_pred, label_encoder.classes_, model_name)\n    \n    # Normalized confusion matrix\n    cm_normalized = plot_confusion_matrix(y_true, y_pred, label_encoder.classes_, model_name, normalize=True)\n    \n    # Macro metrikleri hesapla\n    print(\"Macro metrikler hesaplanıyor...\")\n    macro_metrics = calculate_macro_metrics(y_true, y_pred, label_encoder, model_name)\n    \n    # Bellek temizliği\n    del test_dataset\n    gc.collect()\n    \n    return macro_metrics, cm, y_pred\n\n# Model eğitim fonksiyonu - BELLEK OPTİMİZASYONLU\ndef train_model(model, X_train, y_train, X_test, y_test, tokenizer, class_weights, \n                model_name, max_length=128, batch_size=8, label_encoder=None, y_test_original=None):  # batch_size 16->8\n    \"\"\"Modeli eğitir\"\"\"\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_filename = f\"{model_name}_{timestamp}\"\n    \n    checkpoint_path = f\"outputs/models/{model_filename}.keras\"\n    \n    # BELLEK OPTİMİZASYONU: tf.data.Dataset kullan\n    print(\"Training dataset oluşturuluyor...\")\n    train_dataset = create_tf_dataset_simple(X_train, y_train, tokenizer, batch_size, max_length, shuffle=True)\n    \n    print(\"Validation dataset oluşturuluyor...\")\n    val_dataset = create_tf_dataset_simple(X_test, y_test, tokenizer, batch_size, max_length, shuffle=False)\n    \n    # Steps hesapla\n    steps_per_epoch = len(X_train) // batch_size\n    validation_steps = len(X_test) // batch_size\n    \n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        checkpoint_path,\n        monitor='val_loss',  # f1_score yerine loss\n        verbose=1,\n        save_best_only=True,\n        mode='min'\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_loss',\n        patience=2,  # 3->2\n        verbose=1,\n        restore_best_weights=True\n    )\n    \n    # BELLEK OPTİMİZASYONU: Garbage collection callback\n    class GarbageCollectionCallback(tf.keras.callbacks.Callback):\n        def on_epoch_end(self, epoch, logs=None):\n            gc.collect()\n            tf.keras.backend.clear_session()\n    \n    print(f\"Model eğitimi başlıyor: {model_name}\")\n    \n    model_start_time = time.time()\n    history = model.fit(\n        train_dataset,\n        steps_per_epoch=steps_per_epoch,\n        epochs=2,  # 10->5 (test için)\n        validation_data=val_dataset,\n        validation_steps=validation_steps,\n        callbacks=[checkpoint, early_stop, GarbageCollectionCallback()],\n        verbose=1\n    )\n    model_end_time = time.time()\n    model_train_time = model_end_time - model_start_time\n    \n    history_df = pd.DataFrame(history.history)\n    history_path = f\"outputs/history/{model_filename}_history.csv\"\n    history_df.to_csv(history_path, index=False)\n    print(f\"Eğitim geçmişi kaydedildi: {history_path}\")\n    \n    # Model değerlendirme\n    print(\"Model değerlendiriliyor...\")\n    model_start_time = time.time()\n    evaluation = model.evaluate(val_dataset, steps=validation_steps, verbose=1)\n    model_end_time = time.time()\n    model_test_time = model_end_time - model_start_time\n\n    evaluation_results = dict(zip(model.metrics_names, evaluation))\n    evaluation_results[\"Train Time\"] = model_train_time\n    evaluation_results[\"Test Time\"] = model_test_time\n    \n    print(f\"Değerlendirme sonuçları: {evaluation_results}\")\n    \n    # Confusion matrix ve macro metrikleri hesapla\n    if label_encoder is not None:\n        print(\"\\nConfusion Matrix ve Macro Metrikler hesaplanıyor...\")\n        macro_metrics, cm, y_pred = evaluate_model_with_metrics(\n            model, X_test, y_test, tokenizer, label_encoder, model_name, \n            batch_size=batch_size, max_length=max_length\n        )\n        \n        # Macro metrikleri evaluation_results'a ekle\n        evaluation_results.update(macro_metrics)\n    \n    # Bellek temizliği\n    del train_dataset, val_dataset\n    gc.collect()\n    \n    return history, evaluation_results\n\ndef visualize_metrics(history, model_name):\n    \"\"\"Eğitim metriklerini görselleştirir\"\"\"\n    metrics_dict = history.history if hasattr(history, 'history') else history\n    \n    metrics = [\n        ('loss', 'Model Loss'),\n        ('accuracy', 'Accuracy'),\n    ]\n    \n    print(\"Metrikler görselleştiriliyor...\")\n    \n    for metric, title in metrics:\n        if metric in metrics_dict:\n            plt.figure(figsize=(10, 6))\n            plt.plot(metrics_dict[metric], label=f'Train {title}')\n            val_metric = f'val_{metric}'\n            if val_metric in metrics_dict:\n                plt.plot(metrics_dict[val_metric], label=f'Validation {title}')\n            plt.title(title)\n            plt.xlabel('Epoch')\n            plt.ylabel(title)\n            plt.legend()\n            plt.grid(True)\n            plt.savefig(f\"outputs/images/{model_name}_{metric}.png\")\n            plt.close()\n    \n    print(f\"Metrik görselleştirmeleri kaydedildi\")\n\ndef save_results_to_csv(model_name, eval_results):\n    \"\"\"Model sonuçlarını CSV'ye kaydeder\"\"\"\n    results_path = \"outputs/results.csv\"\n    \n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    result_data = {\n        'model_name': model_name,\n        'timestamp': timestamp,\n        **eval_results\n    }\n    \n    file_exists = os.path.isfile(results_path)\n    \n    with open(results_path, 'a', newline='', encoding='utf-8') as f:\n        writer = csv.DictWriter(f, fieldnames=result_data.keys())\n        \n        if not file_exists:\n            writer.writeheader()\n            \n        writer.writerow(result_data)\n    \n    print(f\"Sonuçlar kaydedildi: {results_path}\")\n\n# Ana fonksiyon\ndef train_news_classification_model(data_path, stopwords_path=None, \n                                    model_name=\"Turkish_BERT_Classifier\",\n                                    max_length=128, batch_size=8, sample_size=None):\n    \"\"\"BERT tabanlı haber sınıflandırma - BELLEK OPTİMİZASYONLU\"\"\"\n    \n    create_directories()\n    check_gpu()\n    \n    # Veriyi yükle - artık y_test_original da dönüyor\n    X_train, X_test, y_train, y_test, class_weights, label_encoder, num_classes, y_test_original = \\\n        load_and_preprocess_data(data_path, stopwords_path=stopwords_path, \n                                max_length=max_length, sample_size=sample_size)\n    \n    # Model oluştur\n    model, tokenizer = create_bert_functional_model(\n        num_classes=num_classes,\n        bert_model_name=\"dbmdz/bert-base-turkish-uncased\",\n        max_length=max_length\n    )\n    \n    # Compile\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']  # Sadece accuracy\n    )\n    \n    print(model.summary())\n    \n    # Eğit - artık label_encoder parametresi de veriliyor\n    history, eval_results = train_model(\n        model, X_train, y_train, X_test, y_test, tokenizer,\n        class_weights, model_name, max_length=max_length, \n        batch_size=batch_size, label_encoder=label_encoder\n    )\n    \n    visualize_metrics(history, model_name)\n    save_results_to_csv(model_name, eval_results)\n    \n    # Belleği temizle\n    tf.keras.backend.clear_session()\n    gc.collect()\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Model Eğitimi Tamamlandı!\")\n    print(f\"Confusion Matrix ve Macro Metrikler kaydedildi!\")\n    print(f\"{'='*50}\\n\")\n    \n    return model, history, eval_results, label_encoder, tokenizer\n\n# KULLANIM\nif __name__ == \"__main__\":\n    try:\n        # BELLEK OPTİMİZASYONU: Küçük parametreler\n        model, history, eval_results, label_encoder, tokenizer = \\\n            train_news_classification_model(\n                data_path=\"/kaggle/input/turkish-news-dataset/cleaned.csv\",\n                stopwords_path='/kaggle/input/turkish-news-dataset/stop-words.txt',\n                model_name=\"BERT\",\n                max_length=128,      # 256->128\n                batch_size=8,        # 16->8\n            )\n        \n        print(\"Model başarıyla eğitildi!\")\n        print(\"Tüm metrikler ve confusion matrix kaydedildi!\")\n        \n    except Exception as e:\n        \n        print(f\"Hata oluştu: {str(e)}\")\n        import traceback\n        print(traceback.format_exc())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"bert\", 'zip', \"./outputs\")\nprint(\"Sonuçlar 'bert.zip' olarak kaydedildi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:24:35.017215Z","iopub.execute_input":"2025-11-20T14:24:35.017722Z","iopub.status.idle":"2025-11-20T14:24:35.123396Z","shell.execute_reply.started":"2025-11-20T14:24:35.017697Z","shell.execute_reply":"2025-11-20T14:24:35.122734Z"}},"outputs":[{"name":"stdout","text":"Sonuçlar 'bert.zip' olarak kaydedildi\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}