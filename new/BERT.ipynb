{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f42509-5cdc-4d30-8827-864cfaec63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "# Transformers kütüphanesini import et\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU kontrol\n",
    "def check_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"{len(gpus)} GPU bulundu:\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  {gpu}\")\n",
    "        for gpu in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                print(f\"GPU bellek büyümesi {gpu} için etkinleştirildi\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"GPU bellek büyümesi {gpu} için ayarlanamadı: {e}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"GPU bulunamadı, işlemler CPU üzerinde gerçekleştirilecek\")\n",
    "        return False\n",
    "\n",
    "# F1 metriği\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=1)\n",
    "        y_true = tf.argmax(y_true, axis=1)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "# Türkçe stopword temizliği\n",
    "def load_turkish_stopwords(stopwords_path):\n",
    "    \"\"\"Türkçe stopword'leri yükler\"\"\"\n",
    "    stoplist = []\n",
    "    try:\n",
    "        with open(stopwords_path, \"r\", encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip() and line.strip()[0:1] != \"#\":\n",
    "                    for word in line.split():\n",
    "                        stoplist.append(word)\n",
    "    except Exception as e:\n",
    "        print(f\"Stopword dosyası yüklenirken hata: {e}\")\n",
    "    return stoplist\n",
    "\n",
    "# Metin temizleme fonksiyonu\n",
    "def clean_turkish_text(text, stoplist=None):\n",
    "    \"\"\"Türkçe metinleri temizler\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = emoji.replace_emoji(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    if stoplist:\n",
    "        words = text.split()\n",
    "        text = ' '.join([word for word in words if word not in stoplist])\n",
    "    \n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Klasörleri oluştur\n",
    "def create_directories():\n",
    "    \"\"\"Gerekli dizin yapısını oluşturur\"\"\"\n",
    "    dirs = [\n",
    "        \"outputs\",\n",
    "        \"outputs/models\",\n",
    "        \"outputs/history\",\n",
    "        \"outputs/images\"\n",
    "    ]\n",
    "    for dir_path in dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    print(\"Dizin yapısı hazırlandı\")\n",
    "\n",
    "# Veri setini yükleme ve ön işleme\n",
    "def load_and_preprocess_data(data_path, text_column=\"Haber Gövdesi Cleaned\", \n",
    "                             label_column=\"Sınıf\", stopwords_path=None, max_length=256):\n",
    "    \"\"\"Veri setini yükler ve ön işler\"\"\"\n",
    "    print(f\"Veri seti yükleniyor: {data_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(data_path, usecols=['Haber Gövdesi Cleaned', 'Sınıf'], encoding='utf-8')\n",
    "        df = df[[text_column, label_column]].copy()\n",
    "        df.columns = ['text', 'label']\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Veri yüklenirken hata oluştu: {str(e)}\")\n",
    "    \n",
    "    stoplist = load_turkish_stopwords(stopwords_path) if stopwords_path else None\n",
    "    \n",
    "    print(\"Metinler temizleniyor...\")\n",
    "    #df['text'] = df['text'].apply(lambda x: clean_turkish_text(x, stoplist))\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(df['label'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    print(f\"Sınıf sayısı: {num_classes}\")\n",
    "    print(f\"Sınıflar: {label_encoder.classes_}\")\n",
    "    \n",
    "    X = df['text']\n",
    "    print(f'NaN sayısı: {X.isna().sum()}')\n",
    "    X = X.dropna()\n",
    "    y_encoded = y_encoded[X.index]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # Sınıf ağırlıklarını hesapla\n",
    "    class_counts = np.bincount(y_train)\n",
    "    total = len(y_train)\n",
    "    class_weights = {i: np.sqrt(total / (num_classes * count)) for i, count in enumerate(class_counts)}\n",
    "    \n",
    "    print(f\"Sınıf ağırlıkları: {class_weights}\")\n",
    "    print(f\"Eğitim seti: {len(X_train)}, Test seti: {len(X_test)}\")\n",
    "    \n",
    "    return X_train, X_test, y_train_onehot, y_test_onehot, class_weights, label_encoder, num_classes\n",
    "\n",
    "# BERT tokenizasyonu\n",
    "def tokenize_texts(texts, tokenizer, max_length=256):\n",
    "    \"\"\"BERT tokenizer ile metinleri işler\"\"\"\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_length,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "# YENİ YAKLAŞIM: Custom BERT Model Sınıfı\n",
    "class BertClassifier(tf.keras.Model):\n",
    "    def __init__(self, bert_model_name, num_classes, max_length=256):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = TFAutoModel.from_pretrained(bert_model_name)\n",
    "        self.bert.trainable = False  # Fine-tuning için False\n",
    "        \n",
    "        # Sınıflandırıcı katmanları\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.2)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Inputs doğrudan metin listesi olacak\n",
    "        if isinstance(inputs, (list, tuple)) and len(inputs) == 2:\n",
    "            # Eğer tokenize edilmiş input gelirse\n",
    "            input_ids, attention_mask = inputs\n",
    "        else:\n",
    "            # Raw text gelirse tokenize et\n",
    "            texts = inputs\n",
    "            encodings = self.tokenizer(\n",
    "                texts.numpy() if hasattr(texts, 'numpy') else texts,\n",
    "                max_length=self.max_length,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            input_ids = encodings['input_ids']\n",
    "            attention_mask = encodings['attention_mask']\n",
    "        \n",
    "        # BERT forward pass\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_outputs.pooler_output\n",
    "        \n",
    "        # Sınıflandırıcı\n",
    "        x = self.dropout1(pooled_output)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout3(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def predict_text(self, text):\n",
    "        \"\"\"Tek bir metin için tahmin yapar\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            [text],\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return self.call((encodings['input_ids'], encodings['attention_mask']))\n",
    "\n",
    "# Model oluşturma - YENİ YAKLAŞIM\n",
    "def create_bert_model(num_classes, bert_model_name=\"dbmdz/bert-base-turkish-uncased\", max_length=256):\n",
    "    \"\"\"Custom BERT modeli oluşturur\"\"\"\n",
    "    print(f\"Custom BERT modeli oluşturuluyor: {bert_model_name}\")\n",
    "    \n",
    "    model = BertClassifier(bert_model_name, num_classes, max_length)\n",
    "    \n",
    "    # Tokenizer'ı ayrıca da döndür\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Alternatif: Functional API ile model\n",
    "def create_bert_functional_model(num_classes, bert_model_name=\"dbmdz/bert-base-turkish-uncased\", max_length=256):\n",
    "    \"\"\"Functional API ile BERT modeli\"\"\"\n",
    "    print(f\"Functional BERT modeli oluşturuluyor: {bert_model_name}\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    \n",
    "    # BERT modeli - önce tokenize, sonra BERT\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # BERT modelini doğrudan çağır\n",
    "    bert_model = TFAutoModel.from_pretrained(bert_model_name)\n",
    "    bert_model.trainable = False\n",
    "    \n",
    "    # BERT outputs\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = bert_output.pooler_output\n",
    "    \n",
    "    # Classification layers\n",
    "    x = tf.keras.layers.Dropout(0.3)(pooled_output)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Model eğitim fonksiyonu\n",
    "def train_model(model, X_train, y_train, X_test, y_test, tokenizer, class_weights, \n",
    "                model_name, max_length=256, use_custom_model=False):\n",
    "    \"\"\"Modeli eğitir, değerlendirir ve sonuçları kaydeder\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"{model_name}_{timestamp}\"\n",
    "    \n",
    "    checkpoint_path = f\"outputs/models/{model_filename}.keras\"\n",
    "    \n",
    "    if use_custom_model:\n",
    "        # Custom model için training loop\n",
    "        print(\"Custom model eğitimi için hazırlanıyor...\")\n",
    "        \n",
    "        # Veriyi hazırla\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train))\n",
    "        train_dataset = train_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test))\n",
    "        test_dataset = test_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Optimizer ve loss\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        # Metrics\n",
    "        train_acc_metric = CategoricalAccuracy()\n",
    "        val_acc_metric = CategoricalAccuracy()\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step(x_batch, y_batch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, predictions)\n",
    "            gradients = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "            train_acc_metric.update_state(y_batch, predictions)\n",
    "            return loss\n",
    "        \n",
    "        @tf.function\n",
    "        def test_step(x_batch, y_batch):\n",
    "            predictions = model(x_batch, training=False)\n",
    "            val_acc_metric.update_state(y_batch, predictions)\n",
    "        \n",
    "        # Training loop\n",
    "        epochs = 10\n",
    "        history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            # Training\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                loss = train_step(x_batch_train, y_batch_train)\n",
    "                if step % 100 == 0:\n",
    "                    print(f\"Step {step}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            train_acc = train_acc_metric.result()\n",
    "            print(f\"Training acc: {train_acc:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            for x_batch_val, y_batch_val in test_dataset:\n",
    "                test_step(x_batch_val, y_batch_val)\n",
    "            \n",
    "            val_acc = val_acc_metric.result()\n",
    "            print(f\"Validation acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # Save history\n",
    "            history['accuracy'].append(float(train_acc))\n",
    "            history['val_accuracy'].append(float(val_acc))\n",
    "            \n",
    "            # Reset metrics\n",
    "            train_acc_metric.reset_state()\n",
    "            val_acc_metric.reset_state()\n",
    "        \n",
    "        # Modeli kaydet\n",
    "        model.save_weights(checkpoint_path.replace('.keras', '_weights.h5'))\n",
    "        \n",
    "        return history, {}, None, None, None, None\n",
    "        \n",
    "    else:\n",
    "        # Standart Functional model eğitimi\n",
    "        print(\"Standart model eğitimi başlıyor...\")\n",
    "        \n",
    "        # Metinleri tokenize et\n",
    "        print(\"Eğitim metinleri tokenize ediliyor...\")\n",
    "        X_train_ids, X_train_mask = tokenize_texts(X_train, tokenizer, max_length)\n",
    "        \n",
    "        print(\"Test metinleri tokenize ediliyor...\")\n",
    "        X_test_ids, X_test_mask = tokenize_texts(X_test, tokenizer, max_length)\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            checkpoint_path,\n",
    "            monitor='val_f1_score',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Model eğitimi başlıyor: {model_name}\")\n",
    "        \n",
    "        model_start_time = time.time()\n",
    "        history = model.fit(\n",
    "            [X_train_ids, X_train_mask], y_train,\n",
    "            batch_size=16,\n",
    "            epochs=10,\n",
    "            validation_data=([X_test_ids, X_test_mask], y_test),\n",
    "            class_weight=class_weights,\n",
    "            callbacks=[checkpoint, early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        model_end_time = time.time()\n",
    "        model_train_time = model_end_time - model_start_time\n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        history_path = f\"outputs/history/{model_filename}_history.csv\"\n",
    "        history_df.to_csv(history_path, index=False)\n",
    "        print(f\"Eğitim geçmişi kaydedildi: {history_path}\")\n",
    "        \n",
    "        # Model değerlendirme\n",
    "        print(\"Model değerlendiriliyor...\")\n",
    "        model_start_time = time.time()\n",
    "        evaluation = model.evaluate([X_test_ids, X_test_mask], y_test, verbose=1)\n",
    "        model_end_time = time.time()\n",
    "        model_test_time = model_end_time - model_start_time\n",
    "\n",
    "        evaluation_results = dict(zip(model.metrics_names, evaluation))\n",
    "        evaluation_results[\"Train Time\"] = model_train_time\n",
    "        evaluation_results[\"Test Time\"] = model_test_time\n",
    "        \n",
    "        print(f\"Değerlendirme sonuçları: {evaluation_results}\")\n",
    "        \n",
    "        visualize_metrics(history, model_filename)\n",
    "        create_confusion_matrix(model, X_test_ids, X_test_mask, y_test, model_filename)\n",
    "        \n",
    "        save_results_to_csv(model_name, evaluation_results)\n",
    "        \n",
    "        return history, evaluation_results, X_train_ids, X_train_mask, X_test_ids, X_test_mask\n",
    "\n",
    "# Diğer fonksiyonlar aynı kalacak...\n",
    "def visualize_metrics(history, model_name):\n",
    "    \"\"\"Eğitim metriklerini görselleştirir ve kaydeder\"\"\"\n",
    "    if isinstance(history, dict):\n",
    "        # Custom model history\n",
    "        metrics_dict = history\n",
    "    else:\n",
    "        # Keras history\n",
    "        metrics_dict = history.history\n",
    "    \n",
    "    metrics = [\n",
    "        ('loss', 'Model Loss'),\n",
    "        ('accuracy', 'Accuracy'),\n",
    "    ]\n",
    "    \n",
    "    print(\"Metrikler görselleştiriliyor...\")\n",
    "    \n",
    "    for metric, title in metrics:\n",
    "        if metric in metrics_dict:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(metrics_dict[metric], label=f'Train {title}')\n",
    "            val_metric = f'val_{metric}'\n",
    "            if val_metric in metrics_dict:\n",
    "                plt.plot(metrics_dict[val_metric], label=f'Validation {title}')\n",
    "            plt.title(title)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(title)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"outputs/images/{model_name}_{metric}.png\")\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"Metrik görselleştirmeleri 'outputs/images/' dizinine kaydedildi\")\n",
    "\n",
    "def create_confusion_matrix(model, X_test_ids, X_test_mask, y_test, model_name):\n",
    "    \"\"\"Test verileri için confusion matrix oluşturur ve kaydeder\"\"\"\n",
    "    print(\"Confusion Matrix oluşturuluyor...\")\n",
    "    \n",
    "    y_pred = model.predict([X_test_ids, X_test_mask])\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    cm = tf.math.confusion_matrix(y_test_classes, y_pred_classes).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Gerçek Değer')\n",
    "    plt.xlabel('Tahmin')\n",
    "    plt.savefig(f\"outputs/images/{model_name}_confusion_matrix.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Confusion Matrix 'outputs/images/{model_name}_confusion_matrix.png' olarak kaydedildi\")\n",
    "\n",
    "def save_results_to_csv(model_name, eval_results):\n",
    "    \"\"\"Model sonuçlarını CSV dosyasına kaydeder\"\"\"\n",
    "    results_path = \"outputs/results.csv\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    result_data = {\n",
    "        'model_name': model_name,\n",
    "        'timestamp': timestamp,\n",
    "        **eval_results\n",
    "    }\n",
    "    \n",
    "    file_exists = os.path.isfile(results_path)\n",
    "    \n",
    "    with open(results_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=result_data.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "            \n",
    "        writer.writerow(result_data)\n",
    "    \n",
    "    print(f\"Sonuçlar '{results_path}' dosyasına kaydedildi\")\n",
    "\n",
    "# Ana fonksiyon\n",
    "def train_news_classification_model(data_path, stopwords_path=None, model_name=\"Turkish_BERT_Classifier\", use_functional=True):\n",
    "    \"\"\"\n",
    "    BERT tabanlı çok sınıflı haber sınıflandırma modeli eğitimi\n",
    "    \"\"\"\n",
    "    create_directories()\n",
    "    check_gpu()\n",
    "    \n",
    "    # Veriyi yükle ve ön işle\n",
    "    X_train, X_test, y_train, y_test, class_weights, label_encoder, num_classes = \\\n",
    "        load_and_preprocess_data(data_path, stopwords_path=stopwords_path)\n",
    "    \n",
    "    if use_functional:\n",
    "        # Functional API modeli kullan\n",
    "        model, tokenizer = create_bert_functional_model(\n",
    "            num_classes=num_classes,\n",
    "            bert_model_name=\"dbmdz/bert-base-turkish-uncased\",\n",
    "            max_length=256\n",
    "        )\n",
    "        use_custom_model = False\n",
    "        print(\"Functional API modeli kullanılıyor\")\n",
    "    else:\n",
    "        # Custom model kullan\n",
    "        model, tokenizer = create_bert_model(\n",
    "            num_classes=num_classes,\n",
    "            bert_model_name=\"dbmdz/bert-base-turkish-uncased\",\n",
    "            max_length=256\n",
    "        )\n",
    "        use_custom_model = True\n",
    "        print(\"Custom model kullanılıyor\")\n",
    "    \n",
    "    if not use_custom_model:\n",
    "        # Functional model için compile\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "        \n",
    "        metrics = [\n",
    "            CategoricalAccuracy(name='accuracy'),\n",
    "            Precision(name='precision'),\n",
    "            Recall(name='recall'),\n",
    "            F1Score(name='f1_score')\n",
    "        ]\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        print(model.summary())\n",
    "    \n",
    "    # Modeli eğit\n",
    "    history, eval_results, X_train_ids, X_train_mask, X_test_ids, X_test_mask = train_model(\n",
    "        model, X_train, y_train, X_test, y_test, tokenizer,\n",
    "        class_weights, model_name, use_custom_model=use_custom_model\n",
    "    )\n",
    "    \n",
    "    # Belleği temizle\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model Eğitimi Tamamlandı!\")\n",
    "    print(f\"Model Adı: {model_name}\")\n",
    "    print(f\"Sınıf Sayısı: {num_classes}\")\n",
    "    print(f\"Sınıflar: {', '.join(label_encoder.classes_)}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return model, history, eval_results, label_encoder, tokenizer\n",
    "\n",
    "# Örnek kullanım - Functional API ile çalıştır\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Functional API modeli ile eğit\n",
    "        model, history, eval_results, label_encoder, tokenizer = \\\n",
    "            train_news_classification_model(\n",
    "                data_path=\"/mnt/d/work2/turkish-news-classification/data/cleaned.csv\",\n",
    "                stopwords_path='./assets/stop-words.txt',\n",
    "                model_name=\"BERT\",\n",
    "                use_functional=True  # Functional API kullan\n",
    "            )\n",
    "        \n",
    "        print(\"Model başarıyla eğitildi!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f76f0-31b5-46ed-9831-ab21f6520c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
